import os
import torch
import gzip
import json
import ast
from sentence_transformers import SentenceTransformer
import sys

# ================= é…ç½®åŒºåŸŸ =================
# 1. åŸºç¡€è·¯å¾„
BASE_DATA_DIR = '../data/Clothing'

# 2. å…³é”®æ–‡ä»¶è·¯å¾„
# [è¾“å…¥] åŸå§‹å…ƒæ•°æ® (æ–‡ä»¶åé€šå¸¸å¾ˆé•¿ï¼Œè¯·ç¡®è®¤æ‚¨çš„å®é™…æ–‡ä»¶å)
# å¸¸è§æ–‡ä»¶å: meta_Clothing_Shoes_and_Jewelry.json.gz
RAW_META_FILE = os.path.join(BASE_DATA_DIR, 'meta_Clothing_Shoes_and_Jewelry.json.gz')

# [è¾“å…¥] éœ€è¦ä¿®å¤çš„ Item List
BAD_ITEM_LIST = os.path.join(BASE_DATA_DIR, 'item_list.txt')

# [è¾“å‡º]
OUTPUT_TEXT_FILE = os.path.join(BASE_DATA_DIR, 'item_list_fixed.txt')
OUTPUT_EMB_FILE = os.path.join(BASE_DATA_DIR, 'semantic_emb.pt')

# 3. æ¨¡å‹è·¯å¾„
MODEL_PATH = '../src/sentence-t5-base'
device = 'cuda' if torch.cuda.is_available() else 'cpu'


# ===========================================

def check_paths():
    if not os.path.exists(RAW_META_FILE):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°å…ƒæ•°æ®æ–‡ä»¶: {RAW_META_FILE}")
        print("   -> è¯·æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦æ­£ç¡® (ä¾‹å¦‚æ˜¯å¦æ˜¯ meta_Clothing...json.gz)")
        sys.exit(1)
    if not os.path.exists(BAD_ITEM_LIST):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° item_list.txt: {BAD_ITEM_LIST}")
        sys.exit(1)
    print("âœ… è·¯å¾„æ£€æŸ¥é€šè¿‡ã€‚")


def load_clothing_meta():
    """åŒæ¨¡è¯»å– Amazon Clothing å…ƒæ•°æ®"""
    print(f"æ­£åœ¨è¯»å–åŸå§‹å…ƒæ•°æ®: {RAW_META_FILE}")
    meta_dict = {}

    count = 0
    success_count = 0

    with gzip.open(RAW_META_FILE, 'rt', encoding='utf-8') as f:
        for line in f:
            count += 1
            data = None

            # --- åŒæ¨¡è§£æ (å…¼å®¹ JSON å’Œ Python Eval) ---
            try:
                data = json.loads(line)
            except json.JSONDecodeError:
                try:
                    data = ast.literal_eval(line)
                except Exception:
                    pass

            if data is None:
                continue

            # æå–æ•°æ®
            try:
                asin = data.get('asin', '')
                title = data.get('title', '')
                brand = data.get('brand', '')

                # æå–åˆ†ç±» (categories é€šå¸¸æ˜¯åˆ—è¡¨çš„åˆ—è¡¨)
                categories = data.get('categories', [[]])
                cat_str = ""
                if categories and isinstance(categories[0], list):
                    cat_str = " ".join(categories[0])
                elif isinstance(categories, list):
                    cat_str = " ".join(categories)

                # ç»„åˆä¸°å¯Œè¯­ä¹‰
                full_text = f"{title} {brand} {cat_str}".strip()

                if asin:
                    meta_dict[str(asin)] = full_text
                    success_count += 1
            except Exception:
                continue

    print(f"âœ… å…ƒæ•°æ®åŠ è½½å®Œæˆã€‚")
    print(f"   - æ€»è¡Œæ•°: {count}")
    print(f"   - æˆåŠŸè§£æ: {success_count}")
    return meta_dict


def fix_item_list(meta_dict):
    print(f"\nSTEP 1: ä¿®å¤ Item List æ–‡æœ¬")

    fixed_lines = []
    missing_count = 0

    with open(BAD_ITEM_LIST, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    print(f"åŸå§‹ item_list å…± {len(lines)} è¡Œã€‚")

    # âš ï¸ å…³é”®ä¿®æ­£ï¼šæ£€æµ‹å¹¶è·³è¿‡è¡¨å¤´
    start_idx = 0
    if len(lines) > 0 and "org_id" in lines[0]:
        print("â„¹ï¸ æ£€æµ‹åˆ°è¡¨å¤´ 'org_id'ï¼Œå·²è·³è¿‡ç¬¬ä¸€è¡Œã€‚")
        start_idx = 1

    for i in range(start_idx, len(lines)):
        line = lines[i].strip()
        if not line: continue

        parts = line.split()
        org_id = parts[0]  # ASIN

        if org_id in meta_dict:
            real_text = meta_dict[org_id]
        else:
            real_text = f"Unknown Product {org_id}"
            missing_count += 1
            if missing_count <= 5:
                print(f"âš ï¸ è­¦å‘Š: ASIN {org_id} æœªæ‰¾åˆ°å…ƒæ•°æ®ã€‚")

        fixed_lines.append(real_text)

    # å†™å…¥æ–°æ–‡ä»¶
    with open(OUTPUT_TEXT_FILE, 'w', encoding='utf-8') as f:
        for text in fixed_lines:
            f.write(text + '\n')

    print(f"âœ… æ–‡æœ¬ä¿®å¤å®Œæˆï¼æœ‰æ•ˆç‰©å“æ•°: {len(fixed_lines)} (åº”ä¸º 23033)")
    print(f"ç¼ºå¤±å…ƒæ•°æ®æ•°: {missing_count}")

    # é¢„è§ˆ
    print("-" * 30)
    print("é¢„è§ˆå‰ 3 è¡Œå†…å®¹:")
    for k in range(min(3, len(fixed_lines))):
        print(f"[{k}] {fixed_lines[k]}")
    print("-" * 30)

    return fixed_lines


def generate_embeddings(text_list):
    print(f"\nSTEP 2: ç”Ÿæˆè¯­ä¹‰å‘é‡")
    print(f"åŠ è½½æ¨¡å‹: {MODEL_PATH}")

    try:
        model = SentenceTransformer(MODEL_PATH, device=device)
        embeddings = model.encode(text_list, show_progress_bar=True, convert_to_tensor=True)

        print(f"ç”Ÿæˆçš„ Embedding å½¢çŠ¶: {embeddings.shape}")
        torch.save(embeddings.cpu(), OUTPUT_EMB_FILE)
        print(f"âœ… å‘é‡å·²ä¿å­˜è‡³: {OUTPUT_EMB_FILE}")

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")


if __name__ == "__main__":
    check_paths()
    meta_dict = load_clothing_meta()
    clean_texts = fix_item_list(meta_dict)
    if clean_texts:
        generate_embeddings(clean_texts)
        print("\nğŸ‰ Clothing æ•°æ®ä¿®å¤å®Œæˆï¼")
        print("è¯·æ‰§è¡Œæœ€åä¸€æ­¥ï¼š")
        print("1. è¿›å…¥ data/Clothing ç›®å½•")
        print("2. åˆ é™¤æ—§çš„ item_list.txt")
        print("3. é‡å‘½å item_list_fixed.txt -> item_list.txt")
        print("4. å›åˆ° code ç›®å½•è¿è¡Œ: python main.py --dataset Clothing --vq --train_vq")