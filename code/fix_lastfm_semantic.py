import os
import torch
import pandas as pd
from sentence_transformers import SentenceTransformer
import sys

# ================= é…ç½®åŒºåŸŸ (åŸºäºä½ çš„ç›®å½•ç»“æ„) =================
# 1. ç›¸å¯¹è·¯å¾„è®¾ç½® (å‡è®¾è„šæœ¬è¿è¡Œåœ¨ code/ ç›®å½•ä¸‹)
BASE_DATA_DIR = '../data/LastFM'
RAW_META_DIR = os.path.join(BASE_DATA_DIR, 'hetrec2011-lastfm-2k')
LOCAL_MODEL_PATH = '../src/sentence-t5-base'  # æŒ‡å‘ä½ æœ¬åœ°çš„ sentence-t5

# 2. å…³é”®æ–‡ä»¶è·¯å¾„
# [è¾“å…¥] åŸå§‹å…ƒæ•°æ® (åŒ…å«çœŸå®æ­Œæ‰‹åå­—)
RAW_ARTISTS_FILE = os.path.join(RAW_META_DIR, 'artists.dat')
# [è¾“å…¥] å½“å‰é”™è¯¯çš„ ID æ˜ å°„æ–‡ä»¶ (åŒ…å« ID é¡ºåº: org_id remap_id)
BAD_ITEM_LIST = os.path.join(BASE_DATA_DIR, 'item_list.txt')

# [è¾“å‡º] ä¿®å¤åçš„æ–‡ä»¶
OUTPUT_TEXT_FILE = os.path.join(BASE_DATA_DIR, 'item_list_fixed.txt')
OUTPUT_EMB_FILE = os.path.join(BASE_DATA_DIR, 'semantic_emb.pt')


# =============================================================

def check_paths():
    """æ£€æŸ¥æ‰€æœ‰å¿…è¦æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    print("æ­£åœ¨æ£€æŸ¥æ–‡ä»¶è·¯å¾„...")
    if not os.path.exists(RAW_ARTISTS_FILE):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°åŸå§‹å…ƒæ•°æ®æ–‡ä»¶: {RAW_ARTISTS_FILE}")
    if not os.path.exists(BAD_ITEM_LIST):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°éœ€è¦ä¿®å¤çš„ Item List: {BAD_ITEM_LIST}")
    if not os.path.exists(LOCAL_MODEL_PATH):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æœ¬åœ°æ¨¡å‹ç›®å½•: {LOCAL_MODEL_PATH}")
    print("âœ… è·¯å¾„æ£€æŸ¥é€šè¿‡ã€‚")


def load_artist_meta():
    """åŠ è½½åŸå§‹ LastFM æ­Œæ‰‹æ•°æ®: ID -> Name"""
    print(f"æ­£åœ¨è¯»å–åŸå§‹å…ƒæ•°æ®: {RAW_ARTISTS_FILE}")
    meta_dict = {}

    # LastFM artists.dat æ ¼å¼é€šå¸¸ä¸º: id \t name \t url ...
    # å¯èƒ½ä¼šæœ‰ç¼–ç é—®é¢˜ï¼Œå…ˆå°è¯• utf-8ï¼Œä¸è¡Œæ¢ latin-1
    try:
        df = pd.read_csv(RAW_ARTISTS_FILE, sep='\t', usecols=[0, 1], names=['id', 'name'], encoding='utf-8')
    except UnicodeDecodeError:
        print("UTF-8 è¯»å–å¤±è´¥ï¼Œåˆ‡æ¢ä¸º Latin-1 ç¼–ç ...")
        df = pd.read_csv(RAW_ARTISTS_FILE, sep='\t', usecols=[0, 1], names=['id', 'name'], encoding='latin-1')

    # æ„å»ºå­—å…¸: string(id) -> string(name)
    for _, row in df.iterrows():
        meta_dict[str(row['id'])] = str(row['name'])

    print(f"âœ… åŸå§‹å…ƒæ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(meta_dict)} ä¸ªæ­Œæ‰‹ä¿¡æ¯ã€‚")
    return meta_dict


def fix_text_list(meta_dict):
    """æ ¹æ®åæ–‡ä»¶çš„é¡ºåºï¼ŒåŒ¹é…å‡ºæ­£ç¡®çš„æ­Œæ‰‹åå­—"""
    print(f"\nSTEP 1: ä¿®å¤ item_list.txt å†…å®¹")

    fixed_lines = []
    missing_count = 0

    with open(BAD_ITEM_LIST, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    print(f"å½“å‰ item_list.txt å…±æœ‰ {len(lines)} è¡Œ (å³ Total Items)ã€‚")

    # é€è¡Œå¤„ç†ï¼Œä¿æŒé¡ºåºç»å¯¹ä¸å˜
    for i, line in enumerate(lines):
        parts = line.strip().split()
        if not parts:
            continue

        # æ ¼å¼é€šå¸¸æ˜¯: org_id remap_id (ä¾‹å¦‚ "1 0")
        org_id = parts[0]

        if org_id in meta_dict:
            real_name = meta_dict[org_id]
        else:
            # å¦‚æœå…ƒæ•°æ®é‡Œæ‰¾ä¸åˆ°è¿™ä¸ªIDï¼Œç”¨å ä½ç¬¦ï¼Œé¿å…æŠ¥é”™
            real_name = f"Unknown Artist {org_id}"
            missing_count += 1
            if missing_count < 5:  # åªæ‰“å°å‰å‡ ä¸ªç¼ºå¤±çš„
                print(f"âš ï¸ è­¦å‘Š: ID {org_id} åœ¨ artists.dat ä¸­æ‰¾ä¸åˆ°ï¼Œå·²æ›¿æ¢ä¸ºå ä½ç¬¦ã€‚")

        fixed_lines.append(real_name)

    # å†™å…¥æ–°æ–‡ä»¶
    with open(OUTPUT_TEXT_FILE, 'w', encoding='utf-8') as f:
        for name in fixed_lines:
            f.write(name + '\n')

    print(f"âœ… æ–‡æœ¬åˆ—è¡¨ä¿®å¤å®Œæˆï¼ç¼ºå¤±æ•°: {missing_count}")
    print(f"æ–°æ–‡ä»¶å·²ä¿å­˜è‡³: {OUTPUT_TEXT_FILE}")

    # æ‰“å°å‰å‡ è¡Œé¢„è§ˆ
    print(f"--- é¢„è§ˆå‰ 3 è¡Œ ---")
    for k in range(min(3, len(fixed_lines))):
        print(f"ID {k}: {fixed_lines[k]}")
    print("-------------------")

    return fixed_lines


def generate_embeddings(text_list):
    """ä½¿ç”¨æœ¬åœ° Sentence-T5 ç”Ÿæˆ Embedding"""
    print(f"\nSTEP 2: ä½¿ç”¨ Sentence-T5 ç”Ÿæˆè¯­ä¹‰å‘é‡")

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"æ­£åœ¨åŠ è½½æœ¬åœ°æ¨¡å‹: {LOCAL_MODEL_PATH}")
    print(f"è¿è¡Œè®¾å¤‡: {device}")

    # åŠ è½½æœ¬åœ° Sentence-T5
    try:
        model = SentenceTransformer(LOCAL_MODEL_PATH, device=device)
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œè¯·æ£€æŸ¥è·¯å¾„ã€‚é”™è¯¯ä¿¡æ¯: {e}")
        return

    print("å¼€å§‹ç¼–ç  (Encoding)... è¿™å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿ...")
    # encode æ–¹æ³•ä¼šè‡ªåŠ¨å¤„ç† batching
    embeddings = model.encode(text_list, show_progress_bar=True, convert_to_tensor=True)

    print(f"ç”Ÿæˆ Embeddings å½¢çŠ¶: {embeddings.shape}")

    # ä¿å­˜ .pt æ–‡ä»¶
    torch.save(embeddings.cpu(), OUTPUT_EMB_FILE)
    print(f"âœ… è¯­ä¹‰å‘é‡å·²ä¿å­˜è‡³: {OUTPUT_EMB_FILE}")


if __name__ == "__main__":
    try:
        check_paths()

        # 1. åŠ è½½å­—å…¸
        meta_dict = load_artist_meta()

        # 2. ä¿®å¤æ–‡æœ¬
        clean_texts = fix_text_list(meta_dict)

        # 3. ç”Ÿæˆå‘é‡
        if clean_texts:
            generate_embeddings(clean_texts)

        print("\nğŸ‰ ====== å…¨éƒ¨å®Œæˆ ======")
        print("è¯·æ‰§è¡Œæœ€åä¸€æ­¥æ“ä½œï¼š")
        print(f"1. å¤‡ä»½åŸæ–‡ä»¶: rename {BAD_ITEM_LIST} item_list.bak")
        print(f"2. æ›¿æ¢æ–°æ–‡ä»¶: rename {OUTPUT_TEXT_FILE} item_list.txt")
        print("3. é‡æ–°è¿è¡Œ main.py")

    except Exception as e:
        print(f"\nâŒ ç¨‹åºå‘ç”Ÿé”™è¯¯: {e}")